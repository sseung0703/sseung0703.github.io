---
layout: post
title: "2020_4th_quarter"
date: 2020-10-08
image_url: https://user-images.githubusercontent.com/54841617/103462065-176db200-4d66-11eb-8ecd-a455b024a929.png
mathjax: true
comments: true
---

# Locally Free Weight sharing for Network Width Search on ICLR2021 (under review)
## Anonymous authors
### 발표자: 이승현 [[paper link](https://openreview.net/forum?id=S0UdquAnr9k), [presentation material](https://trello-attachments.s3.amazonaws.com/5d15b7297b29f54b88064f86/5fb7312bb8c013445c2d5810/791b4cb9cc8809ed73100c1875b0e358/LFWS.pptx)]
- Neural architecture search (NAS)를 활용하여 filter pruning에서 state-of-the-art (SOTA) 성능을 달성한 논문입니다.
- 기존 NAS 기반 filter pruning들이 coarse한 search space를 가져 optimal sub-network를 search하지 못한다는 문제를 해결하기 위해 locally free weight sharing라는 concept를 제안했습니다.
- Locally free weight sharing은 decision boundary에서 더 fine한 search를 가능하게 하여 더 optimal solution에 가까운 sub-network를 얻을 수 있게 합니다.
- 추가적으로 성능을 향상시키기 위해 min-min optimization, max-max selection 등을 제안하여 기존 기법들과 성능의 차이를 더 벌릴 수 있었습니다.
- 저자들이 논문에서 제시한 성능을 기존 기법들에 비해 확연하게 높은 성능을 보인 다는 점은 확실한 장점이지만, 제안된 기법은 매우 높은 비용을 요구할 것으로 예상됩니다. 하지만 저자들이 이에 대한 분석을 하지 않았다는 점이 아쉬운 점입니다.

<p align="center">
<img width="900" alt="제안된 Locally free weight sharing의 conceptural visualization." src="https://user-images.githubusercontent.com/26036843/103463050-cb723b80-4d6c-11eb-913a-b1138673573d.png">
</p>

# Self-training with Noisy Student improves ImageNet classification on CVPR2020
## Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le
### 발표자: 강진구 [[paper link](https://arxiv.org/pdf/1911.04252.pdf), [presentation material](https://drive.google.com/file/d/1GIAu3CTYjv5Noj6pLGh7em-66MGW_nfW/view)]
- Semi-supervised 방법인 Noisy student training을 적용하여 ImageNet SOTA를 경신한 논문입니다. 
- 수많은 Unlabeled 데이터들을 잘 활용하는 방법을 소개하는데 그 방법이 매우 간단합니다. Teacher model과 Student model을 정의하고 Teacher model이 Unlabeled 데이터에 대해 labeling해서 Student model에게 주면 Student model은 그 이미지와 Teacher model이 labeling해 준 pseudo-label을 해당 이미지의 label로 여기고 학습합니다. 첫 Teacher model은 사람이 labeling한 데이터셋을 사용합니다. 첫 Teacher model이 만들어낸 Student model은 그 다음 generation의 Teacher model이 되어 새로운 Student model을 만들어 냅니다. 이 방법으로 여러번 Generation을 거치면서 점점 더 좋은 성능의 모델이 만들어 집니다.
- Teacher model이 Unlabeled data를 labeling 할 때는 이미지에 noise를 섞지 않고, Student model이 pseudo-label을 학습할 때는 noise를 강하게 섞는 것이 이 기법의 핵심입니다. 또한 generation이 거듭되면서 Student model은 기존 Teacher model보다 더 많은 파라미터를 가진 큰 모델로 정의함으로써 더 어려운 문제를 풀 수 있게 만들어 줍니다. 
<p align="center">
<img width="538" alt="Illustration of the Noisy Student Training." src="https://user-images.githubusercontent.com/25892000/103460520-16368800-4d5a-11eb-8400-4fe5048f2c7e.png">
</p>

***

# What does an End-to-End Dialect Identification Model Learn about Non-dialectal Information? on INTERSPEECH 2020
## Shammur A Chowdhury, Ahmed Ali, Suwon Shon, and James Glass, 2020
### 발표자: 지승훈 [[paper link](http://www.interspeech2020.org/index.php?m=content&c=index&a=show&catid=388&id=401), [presentation material](https://trello-attachments.s3.amazonaws.com/5d15b7297b29f54b88064f86/5fe086f4b5c61f70fcad0eb3/33852bab1b6995e4d8d0bd683ca81ab0/20201223_presentation.pdf)]
- 딥 러닝 모델은 layer들을 지날 수록 주어진 task에 어떻게 가까워 지는지를 사람이 해석할 수 있게(interpretable하게) 나타내고 알아보려는 여러 가지 시도들이 있어 왔습니다. 그것을 end-to-end 방언 식별(dialect classification) 모델을 통해 수행해 본 논문으로, 방언 관련 task인 지역 방언 식별(regional dialect classification), 방언과 관련이 없는 성별 식별(gender classification), 화자 검증(speaker verification)과 언어 식별(language identification), 그리고 음성의 채널 정보를 필요로 하는 채널 식별(channel classification)을 proxy task로 설정하여 방언을 식별하는 모델이 input data를 처리하는 과정에서 방언과 관계 있는 정보와 그렇지 않은 정보를 얼마나 잘 가지고 있는지를 나타내 보고자 했습니다.
- 아랍어의 국가 단위 방언을 식별하는 ADI(Arabic Dialect Identification) 모델은 4개의 CNN layer, 2개의 fully-connected layer로 이루어져 있습니다. 이 모델의 output, 즉 intermediate embedding을 따로 뽑은 후, 앞에서 말한 5개의 proxy task 수행을 목적으로 미리 훈련된 간단한 fully-connected layer 모델에 이들을 input으로 넣었을 때 이들 proxy task 모델의 성능은 어떻게 나타나는지 분석해 봅니다.
- 하지만 이것이 정말로 딥 러닝 모델의 중간 형태를 아는 것이라고 할 수 있는가에는 아직 물음표를 붙일 수밖에 없습니다. 그러나 이와 같은 연구로 우리는 딥 러닝 모델을 이해한다는 것에 한 발짝 더 가까워질 수 있을 것입니다.

<p align="center">
<img alt="Illustrated ADI model and the proxy tasks introduced in the paper." src="https://user-images.githubusercontent.com/54841617/103462065-176db200-4d66-11eb-8ecd-a455b024a929.png">
</p>
